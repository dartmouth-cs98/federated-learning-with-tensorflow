# My Introduction to Federated Learnign with Tensorflow
Normally, in order to conduct data analyses or train machine learning models on user data (such as text messages, photographs taken with the mobile camera, location data, phone usage data, web browsing history, etc.), one would first need to collect data from a large number of mobile devices and place that data on a central server. Once this collection step is complete, only then could a machine learning model be trained on that data or an analysis conducted on that data. However, user data is highly sensitive in nature, and collecting and centralizing these forms of data may consititute and illegal or unethical practice, even when efforts are made to ensure the anonymitiy of data records. 

In 2016, however, Google Developed a new technology to circumvent this problem of user privacy. This strategy, dubbed "Federated Learning," allows companies and researchers to uncover insight from consumer data without ever needing to store that data or know its contents. Normally, a company like Google would collect a load of consumer data, store it in one central location on the cloud, and then train a machine learning model on the centralized dataset. With Federated learning, Google instead sends the machine learning model itself to the devices on which the data resides. In Google’s words: 

“It works like this: your device downloads the current [machine learning] model, improves it by learning from data on your phone, and then summarizes the changes as a small focused update. Only this update to the model is sent to the cloud, using encrypted communication, where it is immediately averaged with other user updates to improve the shared model. All the training data remains on your device, and no individual updates are stored in the cloud.”

In effect, federated learning allows companies to extract insight from consumer data without ever needing to collect or store it.

## My Hack-a-thing
While Federated Learning remained in its infancy in 2016, rising concerns over user privacy and data protections laws such as GDPR and the CCPA have driven companies to embrace federated learning as a more standard means to analyize consumer data. Because of the push towards federated learning and federated analytics, dedicated federated learning APIs have been developed since 2016. Although there are more robust APIS available to conduct federated learning, I choose to learn the Tensorflow Federated Framework for my Hack-a-thing 1.

Tensorflow Federated (TFF) is a simulation environment (does not actually work in production **yet**) which allows users to experiment with federated learning and prototype algorithms which could be later developed into production-grade algorithms. In TFF, data sets are partitioned by user, so that each simulated user holds a distinct subset of the training data for your machine learning problem. With the TFF framework, machine learning models are "sent" to simulated user devices, where they can train locally on each user's data. After local training occurs, the simulated devices send updates back to the simulated "server," which aggregates the updates sent by each user device and creates a model that combines the insights collected by analizing data on each user's device. This framework poses unique challenges, because no machine learning model can "see" more than one user's data at a time. To test the model, the TFF framework selects a subset of users to test the newly updated machine learning model, and uses data on those users phones to test the model's performance. Essentailly, TFF performs a train-test split in which some users act as the training data, while other users act as the testing data.

For my hack-a-thing, I followed and added my own modifications on top of a tutorial put together by Tensorflow desribing how to apply federated learning to a *distributed* MNIST dataset (the MNIST dataset is just a collection of hand-written digits from 0 to 9). In this case, each "user" possesses a few hand-written digits on their respective "device", and users have "labeled" these hand-written digits with the numeric value that each hand-written digit corresponds to. The goal of the problem is to train on the user MNIST data, and create a machine learning model which can analyize a hand-written digit and be able to assess which numeric value the hand-written digit corresponds to.

Leading up to this excercise, I had to learn a considerable ammount about federated learning in order to fully grasp the turorial. Here is the course I took and the blogs/ research papers I read in order to develop a background:
- [Coursera Course on Advanced Deployment Scenarios with Tensorflow (Describes Federated Learning)](https://www.coursera.org/learn/advanced-deployment-scenarios-tensorflow/)
- [Google Blog Post on Federated Learning](https://ai.googleblog.com/2017/04/federated-learning-collaborative.html)
- [Tensorflow Federated API documentation](https://www.tensorflow.org/federated/federated_learning)
- [Towards Federated Learning at Scale](https://arxiv.org/abs/1902.01046)

## Code
I followed [this](https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification) tutorial describing how to perform MNIST Image Classification in Tensorflow Federated for my hack-a-thing, and made several additions/ modifications to the tutorial to further improve my understanding of federated learning and to become more comforatable with implementing machine learning models in this novel framework.

Below, I have left the majority of the tutorial in the notebook so that it can be referenced, and so that the work that I completed on top of this tutorial is more easily understandable. In order to accomplish the MNIST classification problem on this *distributed* MNIST dataset, the tensorflow tutorial I used implemented a highly naive single-layer densely connected neural net (DNN). In order to push handwritten digit images into a simple DNN, images are first vectorized (2D image transformed to 1D). I know, from previous experience, that applying a convolutional neural net (CNN) to the MNIST problem can yeild much higher accuracy than the naive DNN approach. CNNs are built to handle 2D data, and are therefore better suited for the problem at hand.

Although I have some limited experience writing CNN algorithms in the past, and understand their functionality well, I wanted to learn how to construct a machine learning model from scratch in this new Tensorflow Federated framework. Using some of the strategies I learned in this tutorial, therefore, towards the end of this notebook, you will see that I constructed a CNN to learn the solution to this *distributed* MNIST problem. Towards the end of the notebook, I show that my CNN algorithm outperforms the tutorial's naive DNN.

## What I Learned:
Going through this tutorial taught me quite a lot about federated learning. The text blurbs, even more than the code itself, helped to illuminate the underlying mechanisms of federated learning and helped me grasp how these mechanisms are put into practice. Having to build my own CNN from scratch in this new framework was also a great learning experience, since I had build, train, and test the model using new functions and new strategies I hadn't been exposed to before.

## What Didn't Work:
Many of the strategies I attempted to use to build my CNN failed at first. Much of my prior experience in writing ML algorithms did not apply in the tensorflow federated framework, and much of the code I wrote at first failed, or proved incompatible with TFF functions. After some tweaking howeever, I was able to successfully construct, train, and test my CNN model, and was able to significantly outperform Google's naive model. You can see the results at the bottom of this notebook.


**NOTE**: The subsections of this notebook that include only my own contribututions are titled "Alternative Federated Dataset Construction", "Alternative Model Creation with Convolutional Neural Net", "
Alternative Training Process with CNN Model", and "Addition to the Tutorial: Evaluating the CNN Model against the standard DNN Model"